{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-04T07:29:16.480558Z","iopub.execute_input":"2024-05-04T07:29:16.481194Z","iopub.status.idle":"2024-05-04T07:29:16.490792Z","shell.execute_reply.started":"2024-05-04T07:29:16.481166Z","shell.execute_reply":"2024-05-04T07:29:16.489767Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport tensorflow as tf\nimport sklearn\nfrom tqdm import tqdm\ndf=pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\ndf.sample()","metadata":{"execution":{"iopub.status.busy":"2024-05-04T07:29:21.032444Z","iopub.execute_input":"2024-05-04T07:29:21.033250Z","iopub.status.idle":"2024-05-04T07:29:34.312596Z","shell.execute_reply.started":"2024-05-04T07:29:21.033215Z","shell.execute_reply":"2024-05-04T07:29:34.311645Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-05-04 07:29:23.002021: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-04 07:29:23.002196: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-04 07:29:23.123132: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                                  review sentiment\n34115  This is one of the best movies to come from Bo...  positive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>34115</th>\n      <td>This is one of the best movies to come from Bo...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Installing Transformers library\n!pip install transformers ","metadata":{"execution":{"iopub.status.busy":"2024-05-04T07:29:40.491710Z","iopub.execute_input":"2024-05-04T07:29:40.492363Z","iopub.status.idle":"2024-05-04T07:29:54.093621Z","shell.execute_reply.started":"2024-05-04T07:29:40.492332Z","shell.execute_reply":"2024-05-04T07:29:54.092519Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Loading the BERT Classifier and Tokenizer along with Input module\nfrom transformers import BertTokenizer, TFBertForSequenceClassification\nfrom transformers import InputExample, InputFeatures\n\nmodel = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2024-05-04T07:30:09.621605Z","iopub.execute_input":"2024-05-04T07:30:09.622031Z","iopub.status.idle":"2024-05-04T07:30:20.102447Z","shell.execute_reply.started":"2024-05-04T07:30:09.621997Z","shell.execute_reply":"2024-05-04T07:30:20.101661Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fed25e39507949a18ad507971f6dd91d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b88cc2378b94463830de3ecca258156"}},"metadata":{}},{"name":"stderr","text":"All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n\nSome weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d206b0b03bf048088bfd4ef3db4797fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"267479429e414762a601a8c157bb2bbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b53c5cf917b4cc98c21d83702a867d1"}},"metadata":{}}]},{"cell_type":"code","source":"def cat2num(value):\n    if value=='positive': \n        return 1\n    else: \n        return 0\n    \ndf['sentiment']  =  df['sentiment'].apply(cat2num)\ntrain = df[:45000]\ntest = df[45000:]","metadata":{"execution":{"iopub.status.busy":"2024-05-04T07:30:30.137261Z","iopub.execute_input":"2024-05-04T07:30:30.137890Z","iopub.status.idle":"2024-05-04T07:30:30.181436Z","shell.execute_reply.started":"2024-05-04T07:30:30.137860Z","shell.execute_reply":"2024-05-04T07:30:30.180599Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T07:30:36.209394Z","iopub.execute_input":"2024-05-04T07:30:36.210306Z","iopub.status.idle":"2024-05-04T07:30:36.262561Z","shell.execute_reply.started":"2024-05-04T07:30:36.210272Z","shell.execute_reply":"2024-05-04T07:30:36.261558Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Model: \"tf_bert_for_sequence_classification\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n bert (TFBertMainLayer)      multiple                  109482240 \n                                                                 \n dropout_37 (Dropout)        multiple                  0 (unused)\n                                                                 \n classifier (Dense)          multiple                  1538      \n                                                                 \n=================================================================\nTotal params: 109483778 (417.65 MB)\nTrainable params: 109483778 (417.65 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"example='In this Kaggle notebook, I will do sentiment analysis using BERT with Huggingface'\ntokens=tokenizer.tokenize(example)\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\nprint(tokens)\nprint(token_ids)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T07:30:57.298576Z","iopub.execute_input":"2024-05-04T07:30:57.299624Z","iopub.status.idle":"2024-05-04T07:30:57.305830Z","shell.execute_reply.started":"2024-05-04T07:30:57.299574Z","shell.execute_reply":"2024-05-04T07:30:57.304836Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"['in', 'this', 'ka', '##ggle', 'notebook', ',', 'i', 'will', 'do', 'sentiment', 'analysis', 'using', 'bert', 'with', 'hugging', '##face']\n[1999, 2023, 10556, 24679, 14960, 1010, 1045, 2097, 2079, 15792, 4106, 2478, 14324, 2007, 17662, 12172]\n","output_type":"stream"}]},{"cell_type":"code","source":"def convert_data_to_examples(train, test, review, sentiment): \n    train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n                                                          text_a = x[review], \n                                                          label = x[sentiment]), axis = 1)\n\n    validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n                                                          text_a = x[review], \n                                                          label = x[sentiment]), axis = 1,)\n  \n    return train_InputExamples, validation_InputExamples","metadata":{"execution":{"iopub.status.busy":"2024-05-04T07:34:19.360072Z","iopub.execute_input":"2024-05-04T07:34:19.360685Z","iopub.status.idle":"2024-05-04T07:34:19.367254Z","shell.execute_reply.started":"2024-05-04T07:34:19.360653Z","shell.execute_reply":"2024-05-04T07:34:19.366231Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_InputExamples, validation_InputExamples = convert_data_to_examples(train,  test, 'review',  'sentiment')","metadata":{"execution":{"iopub.status.busy":"2024-05-04T07:34:28.327397Z","iopub.execute_input":"2024-05-04T07:34:28.327771Z","iopub.status.idle":"2024-05-04T07:34:29.246773Z","shell.execute_reply.started":"2024-05-04T07:34:28.327736Z","shell.execute_reply":"2024-05-04T07:34:29.245983Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_InputExamples","metadata":{"execution":{"iopub.status.busy":"2024-05-04T07:34:35.546560Z","iopub.execute_input":"2024-05-04T07:34:35.547298Z","iopub.status.idle":"2024-05-04T07:34:35.555491Z","shell.execute_reply.started":"2024-05-04T07:34:35.547265Z","shell.execute_reply":"2024-05-04T07:34:35.554396Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"0        InputExample(guid=None, text_a=\"One of the oth...\n1        InputExample(guid=None, text_a='A wonderful li...\n2        InputExample(guid=None, text_a='I thought this...\n3        InputExample(guid=None, text_a=\"Basically ther...\n4        InputExample(guid=None, text_a='Petter Mattei\\...\n                               ...                        \n44995    InputExample(guid=None, text_a=\"I watched this...\n44996    InputExample(guid=None, text_a=\"I am a sucker ...\n44997    InputExample(guid=None, text_a=\"I am a college...\n44998    InputExample(guid=None, text_a=\"huge Ramones f...\n44999    InputExample(guid=None, text_a='I rented this ...\nLength: 45000, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"validation_InputExamples","metadata":{"execution":{"iopub.status.busy":"2024-05-04T07:34:50.279797Z","iopub.execute_input":"2024-05-04T07:34:50.280137Z","iopub.status.idle":"2024-05-04T07:34:50.287640Z","shell.execute_reply.started":"2024-05-04T07:34:50.280113Z","shell.execute_reply":"2024-05-04T07:34:50.286613Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"45000    InputExample(guid=None, text_a=\"What I enjoyed...\n45001    InputExample(guid=None, text_a='MacArthur is a...\n45002    InputExample(guid=None, text_a='What can I say...\n45003    InputExample(guid=None, text_a='A pretty trans...\n45004    InputExample(guid=None, text_a=\"Even though th...\n                               ...                        \n49995    InputExample(guid=None, text_a=\"I thought this...\n49996    InputExample(guid=None, text_a='Bad plot, bad ...\n49997    InputExample(guid=None, text_a='I am a Catholi...\n49998    InputExample(guid=None, text_a='I\\'m going to ...\n49999    InputExample(guid=None, text_a=\"No one expects...\nLength: 5000, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n    features = [] # -> will hold InputFeatures to be converted later\n\n    for e in tqdm(examples):\n        input_dict = tokenizer.encode_plus(\n            e.text_a,\n            add_special_tokens=True,    # Add 'CLS' and 'SEP'\n            max_length=max_length,    # truncates if len(s) > max_length\n            return_token_type_ids=True,\n            return_attention_mask=True,\n            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n            truncation=True\n        )\n\n        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n        features.append(InputFeatures( input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label) )\n\n    def gen():\n        for f in features:\n            yield (\n                {\n                    \"input_ids\": f.input_ids,\n                    \"attention_mask\": f.attention_mask,\n                    \"token_type_ids\": f.token_type_ids,\n                },\n                f.label,\n            )\n\n    return tf.data.Dataset.from_generator(\n        gen,\n        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n        (\n            {\n                \"input_ids\": tf.TensorShape([None]),\n                \"attention_mask\": tf.TensorShape([None]),\n                \"token_type_ids\": tf.TensorShape([None]),\n            },\n            tf.TensorShape([]),\n        ),\n    )\n\n\nDATA_COLUMN = 'review'\nLABEL_COLUMN = 'sentiment'","metadata":{"execution":{"iopub.status.busy":"2024-05-04T07:35:04.545409Z","iopub.execute_input":"2024-05-04T07:35:04.546111Z","iopub.status.idle":"2024-05-04T07:35:04.555352Z","shell.execute_reply.started":"2024-05-04T07:35:04.546080Z","shell.execute_reply":"2024-05-04T07:35:04.554408Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\ntrain_data = train_data.shuffle(100).batch(32).repeat(2)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T07:35:13.609684Z","iopub.execute_input":"2024-05-04T07:35:13.610405Z","iopub.status.idle":"2024-05-04T07:41:54.226553Z","shell.execute_reply.started":"2024-05-04T07:35:13.610374Z","shell.execute_reply":"2024-05-04T07:41:54.225574Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"  0%|          | 0/45000 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|██████████| 45000/45000 [06:40<00:00, 112.47it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\nvalidation_data = validation_data.batch(32)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T07:41:56.299341Z","iopub.execute_input":"2024-05-04T07:41:56.300190Z","iopub.status.idle":"2024-05-04T07:42:40.657631Z","shell.execute_reply.started":"2024-05-04T07:41:56.300157Z","shell.execute_reply":"2024-05-04T07:42:40.656849Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"100%|██████████| 5000/5000 [00:44<00:00, 112.82it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])","metadata":{"execution":{"iopub.status.busy":"2024-05-04T07:45:43.674988Z","iopub.execute_input":"2024-05-04T07:45:43.676009Z","iopub.status.idle":"2024-05-04T07:45:43.695949Z","shell.execute_reply.started":"2024-05-04T07:45:43.675966Z","shell.execute_reply":"2024-05-04T07:45:43.694788Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"model.fit(train_data, epochs=1, validation_data=validation_data)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T07:45:52.105997Z","iopub.execute_input":"2024-05-04T07:45:52.106889Z","iopub.status.idle":"2024-05-04T08:26:14.475590Z","shell.execute_reply.started":"2024-05-04T07:45:52.106853Z","shell.execute_reply":"2024-05-04T08:26:14.474711Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1714808815.552281     114 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"2814/2814 [==============================] - 2422s 833ms/step - loss: 0.2412 - accuracy: 0.8985 - val_loss: 0.3144 - val_accuracy: 0.8894\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"<tf_keras.src.callbacks.History at 0x79fde88ea710>"},"metadata":{}}]},{"cell_type":"code","source":"pred_sentences = ['worst movie of my life, will never watch movies from this series', 'Wow, blew my mind, what a movie by Marvel, animation and story is amazing']","metadata":{"execution":{"iopub.status.busy":"2024-05-04T08:26:38.913996Z","iopub.execute_input":"2024-05-04T08:26:38.914360Z","iopub.status.idle":"2024-05-04T08:26:38.918634Z","shell.execute_reply.started":"2024-05-04T08:26:38.914330Z","shell.execute_reply":"2024-05-04T08:26:38.917769Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"tf_batch = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')   # we are tokenizing before sending into our trained model\ntf_outputs = model(tf_batch)                                  \ntf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)       # axis=-1, this means that the index that will be returned by argmax will be taken from the *last* axis.\nlabels = ['Negative','Positive']\nlabel = tf.argmax(tf_predictions, axis=1)\nlabel = label.numpy()\nfor i in range(len(pred_sentences)):\n    print(pred_sentences[i], \": \", labels[label[i]])","metadata":{"execution":{"iopub.status.busy":"2024-05-04T08:26:49.388142Z","iopub.execute_input":"2024-05-04T08:26:49.388515Z","iopub.status.idle":"2024-05-04T08:26:49.640744Z","shell.execute_reply.started":"2024-05-04T08:26:49.388485Z","shell.execute_reply":"2024-05-04T08:26:49.639772Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"worst movie of my life, will never watch movies from this series :  Negative\nWow, blew my mind, what a movie by Marvel, animation and story is amazing :  Positive\n","output_type":"stream"}]}]}